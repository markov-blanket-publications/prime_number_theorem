\documentclass{article}

% ready for submission
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}}

\title{The prime number theorem, or the incompressibility of the primes}

\date{February 16, 2021}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\section{An information-theoretic derivation of the prime number theorem}

If we know nothing about the primes in the worst case we may assume that each prime number less than or equal to $N$ is drawn uniformly from $[1,N]$. So our source of primes is:

\begin{equation}
X \sim U([1,N])
\end{equation}

where $H(X) = \ln(N)$ is the Shannon entropy of the uniform distribution.  

Now, given a strictly increasing integer sequence of length $N$, $U_N = \{u_i\}_{i=1}^N$ where $u_i = i$ we may define the \textit{prime encoding} of $U_N$ as the binary sequence $X_N = \{x_i\}_{i=1}^N$ where $x_i =1$ if $u_i$ is prime and
$x_i=0$ otherwise. With no prior knowledge, given that each integer is either prime or not prime,
we have $2^N$ possible prime encodings(i.e. arrangements of the primes) in $[1,N] \subset \mathbb{N}$.

If there are $\pi(N)$ primes less than or equal to $N$ then the average number of bits per arrangement gives us the average amount of information gained from correctly identifying each prime number in $U_N$ as:

\begin{equation}
S_c = \frac{\log_2 (2^N)}{\pi(N)}= \frac{N}{\pi(N)}
\end{equation}

Furthermore, if we assume a maximum entropy distribution over the primes then we would expect that each prime is drawn from a uniform distribution as in (1) so we would expect:

\begin{equation}
S_c = \frac{N}{\pi(N)} \sim \ln(N)
\end{equation}

As for why the natural logarithm appears in (3), we may first note that the base of the logarithm in the Shannon Entropy may be freely chosen without changing its properties. Moreover, given the assumptions if we define $(k,k+1] \subset [1,N]$ the average distance between consecutive primes is given by the sum of weighted distances $l$:

\begin{equation}
\sum_{k=1}^{N-1} \frac{1}{k} \lvert (k,k+1] \rvert = \sum_{k=1}^{N-1} \frac{1}{k} \approx \sum_{l=1}^\lambda l \cdot P_l \approx \ln(N) \tag{4}
\end{equation}

where $P_l = \frac{1}{l} \cdot \sum_{k= \frac{l \cdot (l-1)}{2}}^{\frac{l\cdot (l-1)}{2}+l-1} \frac{1}{k+1}$ and  $\lambda = \frac{\sqrt{1+8(N+1)}-1}{2}$.

This is consistent with the maximum entropy assumption in (1) as there are $k$ distinct ways to sample uniformly from $[1,k]$ and a frequency of $\frac{1}{k}$ associated with the event that a prime lies in $(k-1,k]$. The computation (4) is also consistent with Boltzmann's notion of entropy as a measure of possible arrangements.

There is another useful interpretation of (4). If we break $\sum_{k=1}^{N} \frac{1}{k}$ into $\pi(N)$ disjoint blocks of size $[p_k,p_{k+1}]$ where $p_k,p_{k+1} \in \mathbb{P}$ and $p_1 = 1$:

\begin{equation}
\sum_{k=1}^{N} \frac{1}{k} \approx \sum_{k=1}^{\pi(N)} \sum_{b=p_k}^{p_{k+1}} \frac{1}{b} = \sum_{k=1}^{\pi(N)} (p_{k+1}-p_k)\cdot P(p_k) \approx \ln(N) \tag{5}
\end{equation}

where $P(p_k)= \frac{1}{(p_{k+1}-p_k)} \sum_{b=p_k}^{p_{k+1}} \frac{1}{b}$. So we see that (4) also approximates the expected number of observations per prime where $P(p_k)$ may be interpreted as the probability of a successful observation in a frequentist sense. This is consistent with John Wheeler's \textit{it from bit} interpretation of entropy [5], where the entropy measures the average number of bits(i.e. yes/no questions) per prime number.

Now, given (3),(4) and (5) this implies that the average number of bits per prime number is given by:

\begin{equation}
\frac{N}{\pi(N)} \sim \ln(N)
\end{equation}

which is in complete agreement with the predictions of the prime number theorem.

\section{The Shannon source coding theorem, and the compressibility of the primes}

By the Shannon source coding theorem, we may also infer that $\pi(N)$ primes can't be compressed into fewer than $\pi(N) \cdot \ln(N)$ bits. So the primes are incompressible. In fact, given that the expected Kolmogorov Complexity equals the Shannon entropy for computable probability distributions for a prime encoding of length $N$ we must asymptotically obtain:

\begin{equation}
\mathbb{E}[K(X_N)] \sim \pi(N) \cdot \ln(N) \sim N
\end{equation}

where $K(\cdot)$ is the Kolmogorov Complexity.

We shall now proceed by contradiction. If there is an algorithmic method which may be used to prove that the Riemann Hypothesis is true then we may construct a program of finite length $\textit{ zeta}$ which takes as input a strictly increasing integer sequence of length $N$, $U_N$, and outputs a prime encoding of length $N$, $X_N$, by correctly deciding whether each element in that sequence is prime or not.

By the hypothesis on $\textit{ zeta}$ and $U_N$, an application of the Minimum Description Length principle yields:

\begin{equation}
\mathbb{E}[K(\textit{ zeta} \circ U_N)] \leq -\ln(P(X_N \lvert \textit{zeta} \circ U_N)) + \text{Cst} = \text{Cst} 	
\end{equation}

since $P(X_N \lvert \textit{zeta} \circ U_N) = 1$, as there exists a prime encoding $X_N \in \{0,1\}^N$ such that $\textit{ zeta} \circ U_N = X_N$. So we must also have:

\begin{equation}
\mathbb{E}[K(\textit{ zeta} \circ U_N)] = \mathbb{E}[K(X_N)]
\end{equation}

However, $X_N$ is known to be incompressible due to (5) so we have:

\begin{equation}
\lim_{N \to \infty} \frac{\mathbb{E}[K(\textit{ zeta} \circ U_N)]}{\mathbb{E}[K(X_N)]} \sim \lim_{N \to \infty} \frac{\text{Cst}}{N} = 0
\end{equation}

which is a contradiction.

From this analysis we may conclude that while a single counter-example may be used to prove that the Riemann Hypothesis is false, we can't prove that the Riemann Hypothesis is true. 

\newpage 

\section{Testable predictions}

I have colleagues in the machine learning community that believe their powerful machine learning methods may be used to predict the distribution of primes with much greater accuracy than any of the statistical algorithms developed so far. However, my model implies that independently of the amount of data and computational resources at their disposal, if the best machine learning model predicts the next $N$ primes to be at $\{a_i\}_{i=1}^N \in \mathbb{N}$ then for large $N$ their model's statistical performance will converge to an accuracy that is no better than:

\begin{equation}
\frac{1}{N}\sum_{i=1}^N \frac{1}{a_i}
\end{equation}

and the the scenario where they predict all $N$ primes accurately occurs with a frequency of:

\begin{equation}
\prod_{i=1}^N \frac{1}{a_i}
\end{equation}

\section{Discussion}

Concerning the MaxEnt model I propose it is the simplest model that correctly predicts the average number of bits per prime. In this sense, it is also unique in satisfying Occam's razor or what has been formalised as the Minimum Description Length principle [3]. Furthermore, I can also explain why this model works. It turns out that there is a general connection between maximum entropy distributions and incompressible signals.

The primes are incompressible for two fundamental reasons. First, they are dimensionally independent. By this I mean that we may define the infinite-dimensional vector space:

\begin{equation}
\text{span}(\log \mathbb{P}) = \Big\\{\sum_{i=1}^\infty \alpha_i \log p_i \lvert p_i \in \mathbb{P}, \alpha_i \in \mathbb{Z}\Big\\}
\end{equation}

where $\log \mathbb{Q_+} \subset \text{span}(\log \mathbb{P})$, and $\forall p_j, p_{i \neq j} \in \mathbb{P}, \log p_{i \neq j} \perp \log p_j$ since:

\begin{equation}
\exists \alpha_i \in \mathbb{Z}, \sum_{i=1}^\infty \alpha_i \log p_i = \log p_j \iff \alpha_j = 1 \land \alpha_{i \neq j} = 0
\end{equation}

which implies that prime factorisations are unique.

Second, any mathematical system that is sufficiently powerful to formulate a general theory of computation must contain
arithmetic. Given that the integers have a unique representation in terms of the primes, it follows that all the other types in
such a system are derived types relative to the prime numbers.

\section*{References}

\small

[1] Dániel Schumayer and David A. W. Hutchinson. Physics of the Riemann Hypothesis. Arxiv. 2011.

[2] Doron Zagier. Newman's short proof of the Prime Number Theorem. The American Mathematical Monthly, Vol. 104, No. 8 (Oct., 1997), pp. 705-708

[3] Peter D. Grünwald. The Minimum Description Length Principle
. MIT Press. 2007.

[4] M. Li and P. Vitányi. An Introduction to Kolmogorov Complexity and Its Applications. Graduate Texts in Computer Science. Springer. 1997.

[5] Peter Shor. Shannon’s noiseless coding theorem. lecture notes. 2010.

\end{document}

\end{document}